# üöÄ ETL Automatizado de Dados Fundamentalistas com Apache Airflow e SQL Server

![Python](https://img.shields.io/badge/Python-3.x-blue.svg)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.x-orange.svg)
![SQL Server](https://img.shields.io/badge/SQL%20Server-Integration-red.svg)
![Docker](https://img.shields.io/badge/Docker-Containerization-blue.svg)
![Libraries](https://img.shields.io/badge/Libraries-requests%2C%20beautifulsoup4%2C%20pandas%2C%20pyodbc%2C%20pendulum-brightgreen.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## üìä Vis√£o Geral do Projeto

Este reposit√≥rio apresenta uma solu√ß√£o robusta e automatizada para a coleta (web scraping), tratamento e carga (ETL) de dados fundamentalistas de empresas listadas na bolsa brasileira, utilizando como fonte o site [Fundamentus](http://www.fundamentus.com.br/). A grande inova√ß√£o aqui √© a orquestra√ß√£o de todo o processo atrav√©s do **Apache Airflow**, com a execu√ß√£o em um ambiente **Dockerizado**, e a persist√™ncia dos dados em um banco de dados **SQL Server**, incluindo a atualiza√ß√£o de uma tabela hist√≥rica via stored procedure.

Este projeto √© ideal para quem busca uma solu√ß√£o escal√°vel e agendada para monitoramento de dados financeiros, combinando as melhores pr√°ticas de engenharia de dados.

## ‚ú® Funcionalidades Principais

*   **ETL Automatizado com Apache Airflow:** Orquestra√ß√£o completa do fluxo de dados, desde a extra√ß√£o at√© a carga e transforma√ß√£o final, garantindo execu√ß√µes agendadas e monitoramento centralizado.
*   **Web Scraping Robusto:** Coleta abrangente de dados fundamentalistas para todas as a√ß√µes dispon√≠veis no Fundamentus, incluindo indicadores como P/L, VPA, Margens, Receita L√≠quida, EBIT e muito mais.
*   **Limpeza e Normaliza√ß√£o de Dados:** Sanitiza√ß√£o de nomes de colunas e convers√£o de valores (moedas, porcentagens, datas) para formatos num√©ricos e padronizados, facilitando a an√°lise e a inser√ß√£o no banco de dados.
*   **Integra√ß√£o com SQL Server:**
    *   Carga direta dos dados processados em uma tabela tempor√°ria no SQL Server.
    *   Execu√ß√£o de uma *stored procedure* no SQL Server para mover os dados rec√©m-coletados para uma tabela hist√≥rica, garantindo a rastreabilidade e a evolu√ß√£o dos dados ao longo do tempo.
*   **Containeriza√ß√£o com Docker:** Todo o ambiente (Airflow, PostgreSQL para metadados do Airflow, Redis para Celery, e o pr√≥prio ETL) √© empacotado em cont√™ineres Docker, garantindo portabilidade, isolamento e f√°cil implanta√ß√£o. Resumindo, aqui n√≥s garantimos que quando o c√≥digo for compartilhado n√£o surja a c√©lebre frase: "Na minha m√°quina roda...".
*   **Agendamento Flex√≠vel:** A DAG do Airflow pode ser configurada para rodar em qualquer frequ√™ncia (diariamente, semanalmente, etc.), adaptando-se √†s necessidades de atualiza√ß√£o dos dados.
*   **Logging Detalhado:** Implementa√ß√£o de logs que informam o progresso da coleta, avisos e erros, proporcionando transpar√™ncia e auxiliando na depura√ß√£o e monitoramento via interface do Airflow.
*   **Estrutura Modular:** O c√≥digo √© organizado em fun√ß√µes bem definidas, facilitando a compreens√£o, manuten√ß√£o e poss√≠veis extens√µes.

## ü§ñ Como Funciona (para n√£o programadores)

Imagine este sistema como uma equipe de rob√¥s trabalhando em conjunto:

1.  **O Gerente (Apache Airflow):** √â o c√©rebro da opera√ß√£o. Ele sabe exatamente o que precisa ser feito e quando. Ele acorda todos os dias (ou no hor√°rio que voc√™ definir) e diz: "Hora de coletar os dados das empresas!".
2.  **O Coletor de Dados (Script Python):** Este rob√¥ vai at√© o site do Fundamentus, encontra a lista de todas as empresas e, uma por uma, visita as p√°ginas para "copiar" todas as informa√ß√µes financeiras importantes. Ele tamb√©m organiza e limpa esses dados para que fiquem prontos para o pr√≥ximo passo.
3.  **O Transportador (Script Python):** Assim que os dados s√£o coletados e limpos, este rob√¥ os leva diretamente para um grande "arquivo" no seu banco de dados SQL Server. Ele primeiro limpa o arquivo antigo e depois coloca os dados mais recentes l√°.
4.  **O Historiador (Stored Procedure SQL Server):** Depois que os dados novos est√£o no banco, o Gerente (Airflow) pede para um rob√¥ especial do SQL Server (a *stored procedure*) pegar esses dados e guard√°-los em uma "biblioteca hist√≥rica", onde todas as informa√ß√µes de todos os dias ficam armazenadas. Isso permite que voc√™ veja como os dados mudaram ao longo do tempo.
5.  **A Caixa M√°gica (Docker):** Tudo isso acontece dentro de uma "caixa m√°gica" chamada Docker. Essa caixa garante que todos os rob√¥s tenham tudo o que precisam para trabalhar, sem bagun√ßar seu computador. √â como ter um escrit√≥rio completo e port√°til para seus rob√¥s.

No final, voc√™ ter√° um banco de dados SQL Server sempre atualizado com as informa√ß√µes mais recentes das empresas, e tamb√©m um hist√≥rico completo para an√°lises futuras!

## ‚öôÔ∏è Configura√ß√£o e Uso (para programadores)

### Pr√©-requisitos

*   [Docker Desktop](https://www.docker.com/products/docker-desktop/) (inclui Docker Compose) instalado em sua m√°quina.
*   Uma inst√¢ncia do [SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-downloads) acess√≠vel (pode ser local ou em nuvem).
*   Um usu√°rio e senha para o SQL Server com permiss√µes de `SELECT`, `INSERT`, `DELETE` na tabela de carga e `EXECUTE` na stored procedure.

### Estrutura do Projeto
