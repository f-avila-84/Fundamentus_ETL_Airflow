# üöÄ ETL Automatizado de Dados Fundamentalistas com Apache Airflow e SQL Server

![Python](https://img.shields.io/badge/Python-3.x-blue.svg)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.x-orange.svg)
![SQL Server](https://img.shields.io/badge/SQL%20Server-Integration-red.svg)
![Docker](https://img.shields.io/badge/Docker-Containerization-blue.svg)
![Libraries](https://img.shields.io/badge/Libraries-requests%2C%20beautifulsoup4%2C%20pandas%2C%20pyodbc%2C%20pendulum-brightgreen.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)


## üìä Vis√£o Geral do Projeto

Este reposit√≥rio apresenta uma solu√ß√£o robusta e automatizada para a coleta (web scraping), tratamento e carga (ETL) de dados fundamentalistas de empresas listadas na bolsa brasileira, utilizando como fonte o site [Fundamentus](http://www.fundamentus.com.br/). A grande inova√ß√£o aqui √© a orquestra√ß√£o de todo o processo atrav√©s do **Apache Airflow**, com a execu√ß√£o em um ambiente **Dockerizado**, e a persist√™ncia dos dados em um banco de dados **SQL Server**, incluindo a atualiza√ß√£o de uma tabela hist√≥rica via stored procedure.

Este projeto √© ideal para quem busca uma solu√ß√£o escal√°vel e agendada para monitoramento de dados financeiros, combinando as melhores pr√°ticas de engenharia de dados.


## ‚ú® Funcionalidades Principais

*   **ETL Automatizado com Apache Airflow:** Orquestra√ß√£o completa do fluxo de dados, desde a extra√ß√£o at√© a carga e transforma√ß√£o final, garantindo execu√ß√µes agendadas e monitoramento centralizado.
*   **Web Scraping Robusto:** Coleta abrangente de dados fundamentalistas para todas as a√ß√µes dispon√≠veis no Fundamentus, incluindo indicadores como P/L, VPA, Margens, Receita L√≠quida, EBIT e muito mais.
*   **Limpeza e Normaliza√ß√£o de Dados:** Sanitiza√ß√£o de nomes de colunas e convers√£o de valores (moedas, porcentagens, datas) para formatos num√©ricos e padronizados, facilitando a an√°lise e a inser√ß√£o no banco de dados.
*   **Integra√ß√£o com SQL Server:**
    *   Carga direta dos dados processados em uma tabela tempor√°ria no SQL Server.
    *   Execu√ß√£o de uma *stored procedure* no SQL Server para mover os dados rec√©m-coletados para uma tabela hist√≥rica, garantindo a rastreabilidade e a evolu√ß√£o dos dados ao longo do tempo.
*   **Containeriza√ß√£o com Docker:** Todo o ambiente (Airflow, PostgreSQL para metadados do Airflow, Redis para Celery, e o pr√≥prio ETL) √© empacotado em cont√™ineres Docker, garantindo portabilidade, isolamento e f√°cil implanta√ß√£o. Resumindo, aqui n√≥s garantimos que quando o c√≥digo for compartilhado n√£o surja a c√©lebre frase: "Na minha m√°quina roda...".
*   **Agendamento Flex√≠vel:** A DAG do Airflow pode ser configurada para rodar em qualquer frequ√™ncia (diariamente, semanalmente, etc.), adaptando-se √†s necessidades de atualiza√ß√£o dos dados.
*   **Logging Detalhado:** Implementa√ß√£o de logs que informam o progresso da coleta, avisos e erros, proporcionando transpar√™ncia e auxiliando na depura√ß√£o e monitoramento via interface do Airflow.
*   **Estrutura Modular:** O c√≥digo √© organizado em fun√ß√µes bem definidas, facilitando a compreens√£o, manuten√ß√£o e poss√≠veis extens√µes.


## ü§ñ Como Funciona (para n√£o programadores)

Imagine este sistema como uma equipe de rob√¥s trabalhando em conjunto:

1.  **O Gerente (Apache Airflow):** √â o c√©rebro da opera√ß√£o. Ele sabe exatamente o que precisa ser feito e quando. Ele acorda todos os dias (ou no hor√°rio que voc√™ definir) e diz: "Hora de coletar os dados das empresas!".
2.  **O Coletor de Dados (Script Python):** Este rob√¥ vai at√© o site do Fundamentus, encontra a lista de todas as empresas e, uma por uma, visita as p√°ginas para "copiar" todas as informa√ß√µes financeiras importantes. Ele tamb√©m organiza e limpa esses dados para que fiquem prontos para o pr√≥ximo passo.
3.  **O Transportador (Script Python):** Assim que os dados s√£o coletados e limpos, este rob√¥ os leva diretamente para um grande "arquivo" no seu banco de dados SQL Server. Ele primeiro limpa o arquivo antigo e depois coloca os dados mais recentes l√°.
4.  **O Historiador (Stored Procedure SQL Server):** Depois que os dados novos est√£o no banco, o Gerente (Airflow) pede para um rob√¥ especial do SQL Server (a *stored procedure*) pegar esses dados e guard√°-los em uma "biblioteca hist√≥rica", onde todas as informa√ß√µes de todos os dias ficam armazenadas. Isso permite que voc√™ veja como os dados mudaram ao longo do tempo.
5.  **A Caixa M√°gica (Docker):** Tudo isso acontece dentro de uma "caixa m√°gica" chamada Docker. Essa caixa garante que todos os rob√¥s tenham tudo o que precisam para trabalhar, sem bagun√ßar seu computador. √â como ter um escrit√≥rio completo e port√°til para seus rob√¥s.

No final, voc√™ ter√° um banco de dados SQL Server sempre atualizado com as informa√ß√µes mais recentes das empresas, e tamb√©m um hist√≥rico completo para an√°lises futuras!


## ‚öôÔ∏è Configura√ß√£o e Uso (para programadores)

### Pr√©-requisitos

*   [Docker Desktop](https://www.docker.com/products/docker-desktop/) (inclui Docker Compose) instalado em sua m√°quina.
*   Uma inst√¢ncia do [SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-downloads) acess√≠vel (pode ser local ou em nuvem).
*   Um usu√°rio e senha para o SQL Server com permiss√µes de `SELECT`, `INSERT`, `DELETE` na tabela de carga e `EXECUTE` na stored procedure.


### Estrutura do Projeto

‚îú‚îÄ‚îÄ dags/ 

‚îÇ ‚îú‚îÄ‚îÄ fundamentus_etl_with_procedure.py # Defini√ß√£o da DAG do Airflow 

‚îÇ ‚îî‚îÄ‚îÄ Fundamentus_WebScraping_Tratamento_CargaSQL.py # L√≥gica principal do ETL 

‚îú‚îÄ‚îÄ data/ # Pasta para CSVs tempor√°rios (montada como volume Docker) 

‚îú‚îÄ‚îÄ logs/ # Logs do Airflow (montada como volume Docker) 

‚îú‚îÄ‚îÄ plugins/ # Plugins do Airflow (montada como volume Docker) 

‚îú‚îÄ‚îÄ docker-compose.yaml # Configura√ß√£o dos servi√ßos Docker 

‚îî‚îÄ‚îÄ Dockerfile # (Impl√≠cito pelo docker-compose build) Para construir a imagem customizada do Airflow


### Instala√ß√£o e Execu√ß√£o

1.  **Clone o Reposit√≥rio:**
    ```bash
    git clone https://github.com/f-avila-84/Fundamentus_ETL_Airflow.git
    cd Fundamentus_ETL_Airflow
    ```

2.  **Construa a Imagem Docker do Airflow e Inicie os Servi√ßos:**
    Este comando ir√° construir a imagem customizada do Airflow (que incluir√° as depend√™ncias Python como `requests`, `beautifulsoup4`, `pandas`, `pyodbc`, `pendulum` e o driver ODBC para SQL Server) e iniciar todos os servi√ßos definidos no `docker-compose.yaml` (PostgreSQL, Redis, Airflow Webserver, Scheduler, Worker).

    ```bash
    docker compose up -d --build
    ```
    Aguarde alguns minutos para que todos os servi√ßos subam e o Airflow seja inicializado.

3.  **Acesse a Interface do Airflow:**
    Abra seu navegador e acesse `http://localhost:8080`.
    As credenciais padr√£o s√£o:
    *   **Usu√°rio:** `airflow`
    *   **Senha:** `airflow`
    (Voc√™ pode alterar estas credenciais no `docker-compose.yaml` antes de iniciar os servi√ßos).

4.  **Configure a Conex√£o com o SQL Server no Airflow:**
    Dentro da interface do Airflow:
    *   V√° em `Admin` -> `Connections`.
    *   Clique no bot√£o `+` para adicionar uma nova conex√£o.
    *   Preencha os campos conforme abaixo:
        *   **Conn Id:** `sql_server_fundamentus_conn` (Este ID √© crucial e deve ser exatamente este, pois √© referenciado no c√≥digo Python).
        *   **Conn Type:** `Microsoft SQL Server` (Caso n√£o exista esta op√ß√£o, selecione `Generic`)
        *   **Host:** O endere√ßo IP ou hostname do seu SQL Server (ex: `localhost`, `192.168.1.100`, ou o nome do servi√ßo Docker se o SQL Server estiver em outro cont√™iner na mesma rede).
        *   **Schema:** O nome do seu banco de dados (ex: `FundamentosDB`).
        *   **Login:** O nome de usu√°rio para acessar o SQL Server.
        *   **Password:** A senha do usu√°rio.
        *   **Extra:** Adicione o seguinte JSON para configurar o driver ODBC e a criptografia (ajuste o nome do driver se necess√°rio):
            ```json
            {
              "driver": "{ODBC Driver 18 for SQL Server}",
              "encrypt": "yes",
              "trust_server_certificate": "yes"
            }
            ```
    *   Clique em `Test` para verificar a conex√£o e depois em `Save`.

5.  **Habilite a DAG:**
    Na p√°gina inicial do Airflow (DAGs), procure por `fundamentus_etl_with_procedure`.
    Ative a DAG clicando no bot√£o de `toggle` (deve mudar de cinza para azul).

6.  **Execute a DAG:**
    Voc√™ pode aguardar o agendamento (`schedule_interval`) ou acionar a DAG manualmente clicando no bot√£o `Trigger DAG` (√≠cone de play).

### SQL Server Schema Esperado

Para que o ETL funcione corretamente, voc√™ precisar√° de uma tabela de carga e uma stored procedure no seu SQL Server.

**Tabela de Carga (Exemplo):**
A tabela `carga_fundamentus` ser√° criada ou limpa e preenchida a cada execu√ß√£o.
```sql
CREATE TABLE [dbo].[carga_fundamentus](
	[ticker] [varchar](max) NULL,
	[data_execucao] [date] NULL,
	[hora_execucao] [varchar](max) NULL,
	[tipo] [varchar](50) NULL,
	[empresa] [nvarchar](100) NULL,
	[setor] [nvarchar](100) NULL,
	[subsetor] [nvarchar](100) NULL,
	[data_ult_cot] [date] NULL,
	[cotacao] [float] NULL,
	[min_52_sem] [float] NULL,
	[max_52_sem] [float] NULL,
	[vol_med_2m] [float] NULL,
	[ult_balanco_processado] [date] NULL,
	[valor_de_mercado] [float] NULL,
	[valor_da_firma] [float] NULL,
	[nro_acoes] [float] NULL,
	[dia] [float] NULL,
	[pl] [float] NULL,
	[lpa] [float] NULL,
	[mes] [float] NULL,
	[pvp] [float] NULL,
	[vpa] [float] NULL,
	[30_dias] [float] NULL,
	[pebit] [float] NULL,
	[marg_bruta] [float] NULL,
	[12_meses] [float] NULL,
	[psr] [float] NULL,
	[marg_ebit] [float] NULL,
	[pativos] [float] NULL,
	[marg_liquida] [float] NULL,
	[pcap_giro] [float] NULL,
	[ebit_ativo] [float] NULL,
	[pativ_circ_liq] [float] NULL,
	[roic] [float] NULL,
	[div_yield] [float] NULL,
	[roe] [float] NULL,
	[ev_ebitda] [float] NULL,
	[liquidez_corr] [float] NULL,
	[ev_ebit] [float] NULL,
	[div_br_patrim] [float] NULL,
	[cres_rec_5a] [float] NULL,
	[giro_ativos] [float] NULL,
	[ativo] [float] NULL,
	[div_bruta] [float] NULL,
	[disponibilidades] [float] NULL,
	[div_liquida] [float] NULL,
	[ativo_circulante] [float] NULL,
	[patrim_liq] [float] NULL,
	[receita_liquida_12m] [float] NULL,
	[receita_liquida_3m] [float] NULL,
	[ebit_12m] [float] NULL,
	[ebit_3m] [float] NULL,
	[lucro_liquido_12m] [float] NULL,
	[lucro_liquido_3m] [float] NULL,
	[depositos] [float] NULL,
	[cart_de_credito] [float] NULL,
	[result_int_financ_12m] [float] NULL,
	[result_int_financ_3m] [float] NULL,
	[rec_servicos_12m] [float] NULL,
	[rec_servicos_3m] [float] NULL,
	[2010] [float] NULL,
	[2011] [float] NULL,
	[2012] [float] NULL,
	[2013] [float] NULL,
	[2014] [float] NULL,
	[2015] [float] NULL,
	[2016] [float] NULL,
	[2017] [float] NULL,
	[2018] [float] NULL,
	[2019] [float] NULL,
	[2020] [float] NULL,
	[2021] [float] NULL,
	[2022] [float] NULL,
	[2023] [float] NULL,
	[2024] [float] NULL,
	[2025] [float] NULL
); 
```

**Stored Procedure (Exemplo):** A stored procedure carga_fundamentus_historico ser√° respons√°vel por mover os dados da tabela de carga para sua tabela hist√≥rica, adicionando um id √∫nico e garantindo que n√£o haja duplicatas para a mesma data de execu√ß√£o/ticker.

```sql
CREATE PROCEDURE [dbo].[carga_fundamentus_historico]
    AS

-- Evitar duplicidade de dados de um mesmo dia.
-- Exclui dados da tabela historica que possuam a mesma data de execu√ß√£o da tabela de carga.
DELETE FROM [FundamentosDB].[dbo].[fundamentus_historico]
WHERE data_execucao IN (SELECT DISTINCT data_execucao FROM [FundamentosDB].[dbo].[carga_fundamentus])

-- Insere os novos dados na tabela historica.
INSERT INTO [FundamentosDB].[dbo].[fundamentus_historico]
SELECT *
  FROM [FundamentosDB].[dbo].[carga_fundamentus]

    GO;
```


### ü§ù Contribuindo
Este √© um projeto desenvolvido para fins de estudo e portf√≥lio. No momento, n√£o estou buscando contribui√ß√µes externas. No entanto, sinta-se √† vontade para fazer um fork, explorar e adaptar o c√≥digo para suas necessidades!


### üìß Autor / Contato
Desenvolvido por Felipe Avila.

[github.com](https://github.com/f-avila-84)

[linkedin.com](https://www.linkedin.com/in/avilafelipe/)



### ‚ö†Ô∏è Aviso Legal e Disclaimer de Investimento
Este c√≥digo foi desenvolvido para fins educacionais e informativos como parte de um projeto de estudo pessoal. Ele tem como objetivo demonstrar a coleta e organiza√ß√£o de dados fundamentalistas de forma automatizada.

√â fundamental entender que as informa√ß√µes obtidas atrav√©s deste script N√ÉO constituem aconselhamento financeiro, recomenda√ß√£o de investimento ou endosso de qualquer tipo de estrat√©gia de investimento. O mercado financeiro √© complexo e investimentos envolvem riscos, incluindo a possibilidade de perda de capital.

N√£o se baseie unicamente nos dados gerados por este c√≥digo para tomar decis√µes de investimento.
Retornos passados n√£o s√£o garantia de retornos futuros.
Qualquer decis√£o de investimento √© de sua inteira responsabilidade.
Recomenda-se sempre consultar um profissional financeiro qualificado antes de tomar qualquer decis√£o de investimento. O desenvolvedor deste c√≥digo n√£o se responsabiliza por quaisquer perdas ou preju√≠zos decorrentes do uso ou interpreta√ß√£o das informa√ß√µes aqui contidas.

### üìú Licen√ßa
Este projeto est√° licenciado sob a licen√ßa MIT. Veja o arquivo LICENSE para mais detalhes. 

[opensource.org](https://opensource.org/license/mit)





